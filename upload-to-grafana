#!/usr/bin/env python3
import argparse
import json
import requests
import sys
import subprocess

NAME = sys.argv[0].split('/')[-1]
RELATIVE_PATH = '/'.join(sys.argv[0].split('/')[:-1])

cli = argparse.ArgumentParser(
    description=f"`{NAME}` is an admin tool for CRU operations on Grafana dashboards and datasources."
)
cli.add_argument(
    '--user',
    dest="user",
    type=str,
    required=True,
    help="Grafana admin username"
)
cli.add_argument(
    '--password',
    dest="password",
    type=str,
    required=True,
    help="Grafana admin password"
)
cli.add_argument(
    '--env',
    dest="env",
    choices=["dev", "integration", "staging", "prod"],
    required=True,
    help="Deployment environment"
)

with open(RELATIVE_PATH + '/fogg.json', 'r') as fh:
    module_names = json.load(fh)['modules'].keys()

cli.add_argument(
    'module',
    choices=module_names,
    help="Deployment environment"
)

options = cli.parse_args(sys.argv[1:])

domain = {
    "dev": ".dev",
    "integration": ".dev",
    "staging": ".dev",
    "prod": "",
}[options.env]

base_url = "https://{}:{}@metrics{}.data.humancellatlas.org/api/".format(
    options.user,
    options.password,
    domain
)

def log_response(response, url):
    print("{} {}".format(response.status_code, url))
    print(json.dumps(json.loads(response.content), sort_keys=True, indent=4))


def upload_datasource(base_url, datasource):
    url = base_url + "datasources"
    response = requests.get(
        url + "/name/" + datasource['name'],
        headers={
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
    )
    id = json.loads(response.content).get('id')
    log_response(response, url)
    url += f"/{id}" if id else ""
    method = "PUT" if id else "POST"
    response = requests.request(
        method,
        url,
        data=json.dumps(datasource),
        headers={
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
    )
    log_response(response, url)


def upload_dashboard(base_url, dashboard):
    url = base_url + "dashboards/db"
    body = '{"dashboard":' + json.dumps(dashboard) + ', "overwrite": true}'

    response = requests.post(
        url,
        data=body,
        headers={
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }
    )
    log_response(response, url)


def get_terraform_output(env, module, output_name):
    cmd = f"cd {RELATIVE_PATH}/terraform/envs/{env}/{module} && terraform output {output_name}"
    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)
    out, err = p.communicate()
    return json.loads(out)


datasources = get_terraform_output(options.env, options.module, 'datasources')

for datasource in datasources:
    upload_datasource(base_url, datasource)


dashboards = get_terraform_output(options.env, options.module, 'dashboards')

for dashboard in dashboards:
    upload_dashboard(base_url, dashboard)
